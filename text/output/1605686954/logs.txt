{'data_path': './data', 'classifier_path': './models', 'kenlm_path': './models/kenlm', 'outf': '1605686954', 'vocab_size': 11000, 'maxlen': 10, 'lowercase': True, 'packed_rep': True, 'emsize': 300, 'nhidden': 300, 'nlayers': 1, 'noise_radius': 0.2, 'noise_anneal': 0.995, 'hidden_init': False, 'arch_i': '300-300', 'arch_g': '300-300', 'arch_d': '300-300', 'arch_conv_filters': '500-700-1000', 'arch_conv_strides': '1-2-2', 'arch_conv_windows': '3-3-3', 'z_size': 100, 'temp': 1, 'enc_grad_norm': True, 'gan_toenc': -0.01, 'dropout': 0.0, 'useJS': True, 'perturb_z': True, 'epochs': 15, 'min_epochs': 20, 'no_earlystopping': False, 'patience': 5, 'batch_size': 32, 'niters_ae': 1, 'niters_gan_d': 5, 'niters_gan_g': 1, 'niters_inv': 5, 'niters_gan_schedule': '2-4-6', 'lr_ae': 1, 'lr_inv': 1e-05, 'lr_gan_g': 5e-05, 'lr_gan_d': 1e-05, 'beta1': 0.9, 'clip': 1, 'gan_clamp': 0.01, 'convolution_enc': False, 'use_inv_ae': False, 'update_base': False, 'load_pretrained': '1605104206', 'reload_exp': None, 'sample': False, 'N': 5, 'log_interval': 200, 'seed': 1111, 'cuda': True, 'debug_mode': False, 'hybrid': False, 'ntokens': 11004}
Create experiment at ./output/1605104206
Starting with epoch :1
Training...
[1/15][99/13866] Loss_I: 878.08947754 
[1/15][199/13866] Loss_I: 377.68218994 
[1/15][299/13866] Loss_I: 229.84446716 
[1/15][399/13866] Loss_I: 152.73277283 
[1/15][499/13866] Loss_I: 113.79000092 
[1/15][599/13866] Loss_I: 84.36438751 
[1/15][699/13866] Loss_I: 67.88034058 
[1/15][799/13866] Loss_I: 51.72872162 
[1/15][899/13866] Loss_I: 40.29335785 
[1/15][999/13866] Loss_I: 29.51335907 
[1/15][1099/13866] Loss_I: 23.18390465 
[1/15][1199/13866] Loss_I: 17.91491699 
[1/15][1299/13866] Loss_I: 14.11900043 
[1/15][1399/13866] Loss_I: 12.45196629 
[1/15][1499/13866] Loss_I: 9.37107944 
[1/15][1599/13866] Loss_I: 8.86255836 
[1/15][1699/13866] Loss_I: 6.45238113 
[1/15][1799/13866] Loss_I: 5.94185495 
[1/15][1899/13866] Loss_I: 5.00321007 
[1/15][1999/13866] Loss_I: 4.20194960 
[1/15][2099/13866] Loss_I: 3.45640898 
[1/15][2199/13866] Loss_I: 2.90239739 
[1/15][2299/13866] Loss_I: 2.64898109 
[1/15][2399/13866] Loss_I: 2.08794236 
[1/15][2499/13866] Loss_I: 1.93973577 
[1/15][2599/13866] Loss_I: 1.52962530 
[1/15][2699/13866] Loss_I: 1.29251087 
[1/15][2799/13866] Loss_I: 1.16254628 
[1/15][2899/13866] Loss_I: 0.91816634 
[1/15][2999/13866] Loss_I: 0.77192694 
[1/15][3099/13866] Loss_I: 0.60853505 
[1/15][3199/13866] Loss_I: 0.52325606 
[1/15][3299/13866] Loss_I: 0.43429783 
[1/15][3399/13866] Loss_I: 0.31553996 
[1/15][3499/13866] Loss_I: 0.25766075 
[1/15][3599/13866] Loss_I: 0.20856474 
[1/15][3699/13866] Loss_I: 0.15453506 
[1/15][3799/13866] Loss_I: 0.13202928 
[1/15][3899/13866] Loss_I: 0.09355706 
[1/15][3999/13866] Loss_I: 0.07981319 
[1/15][4099/13866] Loss_I: 0.05788057 
[1/15][4199/13866] Loss_I: 0.04031966 
[1/15][4299/13866] Loss_I: 0.03699700 
[1/15][4399/13866] Loss_I: 0.02927184 
[1/15][4499/13866] Loss_I: 0.02490138 
[1/15][4599/13866] Loss_I: 0.02327290 
[1/15][4699/13866] Loss_I: 0.02273632 
[1/15][4799/13866] Loss_I: 0.02296310 
[1/15][4899/13866] Loss_I: 0.02181328 
[1/15][4999/13866] Loss_I: 0.02261350 
[1/15][5099/13866] Loss_I: 0.02211251 
[1/15][5199/13866] Loss_I: 0.02258642 
[1/15][5299/13866] Loss_I: 0.02218537 
[1/15][5399/13866] Loss_I: 0.02262438 
[1/15][5499/13866] Loss_I: 0.02230652 
[1/15][5599/13866] Loss_I: 0.02182712 
[1/15][5699/13866] Loss_I: 0.02214016 
[1/15][5799/13866] Loss_I: 0.02167840 
[1/15][5899/13866] Loss_I: 0.02126366 
[1/15][5999/13866] Loss_I: 0.02190360 
[1/15][6099/13866] Loss_I: 0.02176471 
[1/15][6199/13866] Loss_I: 0.02185668 
[1/15][6299/13866] Loss_I: 0.02160554 
[1/15][6399/13866] Loss_I: 0.02125037 
[1/15][6499/13866] Loss_I: 0.02113884 
[1/15][6599/13866] Loss_I: 0.02081876 
[1/15][6699/13866] Loss_I: 0.02104264 
[1/15][6799/13866] Loss_I: 0.02068751 
[1/15][6899/13866] Loss_I: 0.02097236 
[1/15][6999/13866] Loss_I: 0.02117212 
[1/15][7099/13866] Loss_I: 0.02025861 
[1/15][7199/13866] Loss_I: 0.02051532 
[1/15][7299/13866] Loss_I: 0.02037589 
[1/15][7399/13866] Loss_I: 0.02066869 
[1/15][7499/13866] Loss_I: 0.02055538 
