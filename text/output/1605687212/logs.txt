{'data_path': './data', 'classifier_path': './models', 'kenlm_path': './models/kenlm', 'outf': '1605687212', 'vocab_size': 11000, 'maxlen': 10, 'lowercase': True, 'packed_rep': True, 'emsize': 300, 'nhidden': 300, 'nlayers': 1, 'noise_radius': 0.2, 'noise_anneal': 0.995, 'hidden_init': False, 'arch_i': '300-300', 'arch_g': '300-300', 'arch_d': '300-300', 'arch_conv_filters': '500-700-1000', 'arch_conv_strides': '1-2-2', 'arch_conv_windows': '3-3-3', 'z_size': 100, 'temp': 1, 'enc_grad_norm': True, 'gan_toenc': -0.01, 'dropout': 0.0, 'useJS': True, 'perturb_z': True, 'epochs': 15, 'min_epochs': 20, 'no_earlystopping': False, 'patience': 5, 'batch_size': 32, 'niters_ae': 1, 'niters_gan_d': 5, 'niters_gan_g': 1, 'niters_inv': 5, 'niters_gan_schedule': '2-4-6', 'lr_ae': 1, 'lr_inv': 1e-05, 'lr_gan_g': 5e-05, 'lr_gan_d': 1e-05, 'beta1': 0.9, 'clip': 1, 'gan_clamp': 0.01, 'convolution_enc': False, 'use_inv_ae': False, 'update_base': False, 'load_pretrained': '1605104206', 'reload_exp': None, 'sample': False, 'N': 5, 'log_interval': 200, 'seed': 1111, 'cuda': True, 'debug_mode': False, 'hybrid': False, 'ntokens': 11004}
Create experiment at ./output/1605104206
Starting with epoch :1
Training...
[1/15][99/13866] Loss_I: 878.08947754 
[1/15][199/13866] Loss_I: 377.68218994 
[1/15][299/13866] Loss_I: 229.84446716 
[1/15][399/13866] Loss_I: 152.73277283 
[1/15][499/13866] Loss_I: 113.79000092 
[1/15][599/13866] Loss_I: 84.36438751 
[1/15][699/13866] Loss_I: 67.88034058 
[1/15][799/13866] Loss_I: 51.72872162 
[1/15][899/13866] Loss_I: 40.29335785 
[1/15][999/13866] Loss_I: 29.51335907 
[1/15][1099/13866] Loss_I: 23.18390465 
[1/15][1199/13866] Loss_I: 17.91491699 
[1/15][1299/13866] Loss_I: 14.11900043 
[1/15][1399/13866] Loss_I: 12.45196629 
[1/15][1499/13866] Loss_I: 9.37107944 
[1/15][1599/13866] Loss_I: 8.86255836 
[1/15][1699/13866] Loss_I: 6.45238113 
[1/15][1799/13866] Loss_I: 5.94185495 
[1/15][1899/13866] Loss_I: 5.00321007 
[1/15][1999/13866] Loss_I: 4.20194960 
[1/15][2099/13866] Loss_I: 3.45640898 
[1/15][2199/13866] Loss_I: 2.90239739 
[1/15][2299/13866] Loss_I: 2.64898109 
[1/15][2399/13866] Loss_I: 2.08794236 
[1/15][2499/13866] Loss_I: 1.93973577 
[1/15][2599/13866] Loss_I: 1.52962530 
[1/15][2699/13866] Loss_I: 1.29251087 
[1/15][2799/13866] Loss_I: 1.16254628 
[1/15][2899/13866] Loss_I: 0.91816634 
[1/15][2999/13866] Loss_I: 0.77192694 
[1/15][3099/13866] Loss_I: 0.60853505 
[1/15][3199/13866] Loss_I: 0.52325606 
[1/15][3299/13866] Loss_I: 0.43429783 
[1/15][3399/13866] Loss_I: 0.31553996 
[1/15][3499/13866] Loss_I: 0.25766075 
[1/15][3599/13866] Loss_I: 0.20856474 
[1/15][3699/13866] Loss_I: 0.15453506 
[1/15][3799/13866] Loss_I: 0.13202928 
[1/15][3899/13866] Loss_I: 0.09355706 
[1/15][3999/13866] Loss_I: 0.07981319 
[1/15][4099/13866] Loss_I: 0.05788057 
[1/15][4199/13866] Loss_I: 0.04031966 
[1/15][4299/13866] Loss_I: 0.03699700 
[1/15][4399/13866] Loss_I: 0.02927184 
[1/15][4499/13866] Loss_I: 0.02490138 
[1/15][4599/13866] Loss_I: 0.02327290 
[1/15][4699/13866] Loss_I: 0.02273632 
[1/15][4799/13866] Loss_I: 0.02296310 
[1/15][4899/13866] Loss_I: 0.02181328 
[1/15][4999/13866] Loss_I: 0.02261350 
[1/15][5099/13866] Loss_I: 0.02211251 
[1/15][5199/13866] Loss_I: 0.02258642 
[1/15][5299/13866] Loss_I: 0.02218537 
[1/15][5399/13866] Loss_I: 0.02262438 
[1/15][5499/13866] Loss_I: 0.02230652 
[1/15][5599/13866] Loss_I: 0.02182712 
[1/15][5699/13866] Loss_I: 0.02214016 
[1/15][5799/13866] Loss_I: 0.02167840 
[1/15][5899/13866] Loss_I: 0.02126366 
[1/15][5999/13866] Loss_I: 0.02190360 
[1/15][6099/13866] Loss_I: 0.02176471 
[1/15][6199/13866] Loss_I: 0.02185668 
[1/15][6299/13866] Loss_I: 0.02160554 
[1/15][6399/13866] Loss_I: 0.02125037 
[1/15][6499/13866] Loss_I: 0.02113884 
[1/15][6599/13866] Loss_I: 0.02081876 
[1/15][6699/13866] Loss_I: 0.02104264 
[1/15][6799/13866] Loss_I: 0.02068751 
[1/15][6899/13866] Loss_I: 0.02097236 
[1/15][6999/13866] Loss_I: 0.02117212 
[1/15][7099/13866] Loss_I: 0.02025861 
[1/15][7199/13866] Loss_I: 0.02051532 
[1/15][7299/13866] Loss_I: 0.02037589 
[1/15][7399/13866] Loss_I: 0.02066869 
[1/15][7499/13866] Loss_I: 0.02055538 
[1/15][7599/13866] Loss_I: 0.02043130 
[1/15][7699/13866] Loss_I: 0.02068346 
[1/15][7799/13866] Loss_I: 0.02064601 
[1/15][7899/13866] Loss_I: 0.02077996 
[1/15][7999/13866] Loss_I: 0.02010407 
[1/15][8099/13866] Loss_I: 0.02058924 
[1/15][8199/13866] Loss_I: 0.02098833 
[1/15][8299/13866] Loss_I: 0.02017786 
[1/15][8399/13866] Loss_I: 0.01969294 
[1/15][8499/13866] Loss_I: 0.01972520 
[1/15][8599/13866] Loss_I: 0.02031793 
[1/15][8699/13866] Loss_I: 0.01969789 
[1/15][8799/13866] Loss_I: 0.01943331 
[1/15][8899/13866] Loss_I: 0.01948410 
[1/15][8999/13866] Loss_I: 0.02054818 
[1/15][9099/13866] Loss_I: 0.01973506 
[1/15][9199/13866] Loss_I: 0.01937634 
[1/15][9299/13866] Loss_I: 0.01942354 
[1/15][9399/13866] Loss_I: 0.01946375 
[1/15][9499/13866] Loss_I: 0.01983791 
[1/15][9599/13866] Loss_I: 0.02000986 
[1/15][9699/13866] Loss_I: 0.01929498 
[1/15][9799/13866] Loss_I: 0.01958810 
[1/15][9899/13866] Loss_I: 0.01903593 
[1/15][9999/13866] Loss_I: 0.01986840 
[1/15][10099/13866] Loss_I: 0.01952730 
[1/15][10199/13866] Loss_I: 0.02047794 
[1/15][10299/13866] Loss_I: 0.01947130 
[1/15][10399/13866] Loss_I: 0.01965635 
[1/15][10499/13866] Loss_I: 0.01881145 
[1/15][10599/13866] Loss_I: 0.01961615 
[1/15][10699/13866] Loss_I: 0.01957342 
[1/15][10799/13866] Loss_I: 0.01955906 
[1/15][10899/13866] Loss_I: 0.01876557 
[1/15][10999/13866] Loss_I: 0.01950352 
[1/15][11099/13866] Loss_I: 0.01902162 
[1/15][11199/13866] Loss_I: 0.01954723 
[1/15][11299/13866] Loss_I: 0.02036539 
[1/15][11399/13866] Loss_I: 0.01927164 
[1/15][11499/13866] Loss_I: 0.01932108 
[1/15][11599/13866] Loss_I: 0.01942806 
[1/15][11699/13866] Loss_I: 0.01906092 
[1/15][11799/13866] Loss_I: 0.01922057 
[1/15][11899/13866] Loss_I: 0.01892194 
[1/15][11999/13866] Loss_I: 0.01950148 
[1/15][12099/13866] Loss_I: 0.01891296 
[1/15][12199/13866] Loss_I: 0.01950669 
[1/15][12299/13866] Loss_I: 0.01959135 
[1/15][12399/13866] Loss_I: 0.01963866 
[1/15][12499/13866] Loss_I: 0.01960023 
[1/15][12599/13866] Loss_I: 0.01897537 
[1/15][12699/13866] Loss_I: 0.01937590 
[1/15][12799/13866] Loss_I: 0.01907063 
[1/15][12899/13866] Loss_I: 0.01958548 
[1/15][12999/13866] Loss_I: 0.01917511 
[1/15][13099/13866] Loss_I: 0.01934484 
[1/15][13199/13866] Loss_I: 0.01877748 
